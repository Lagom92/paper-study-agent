import time
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote import logging
from dotenv import load_dotenv


def main():
    # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (API Key ë“±)
    load_dotenv()

    # LangSmith í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì • (ë¶„ì„ ë° ì¶”ì ìš©)
    logging.langsmith(project_name="Paper-bot")

    # Streamlit í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
    st.set_page_config(page_title="Chatbot", page_icon="ğŸ¤–")
    st.title("ğŸ¤– Chatbot")
    st.write("ê¸°ë³¸ LLM ì±—ë´‡")

    # ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ì´ë ¥ì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™” (ì²« ë©”ì‹œì§€: ì±—ë´‡ ì¸ì‚¬)
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
        ]

    # ì´ì „ ëŒ€í™” ë©”ì‹œì§€ í™”ë©´ì— ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    if user_input := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ ë° í™”ë©´ì— ì¶œë ¥
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # ì±—ë´‡ ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
        with st.chat_message("assistant"):
            message_placeholder = st.empty()  # ì‹¤ì‹œê°„ ì‘ë‹µ ì¶œë ¥ì„ ìœ„í•œ placeholder
            full_response = ""

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
            prompt = PromptTemplate.from_template(
                """ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ì±—ë´‡ì´ì•¼. 
            í•œêµ­ë§ë¡œ ëŒ€ë‹µí•´ì¤˜.

            #Question:
            {question}

            #Answer:"""
            )

            # LLM ê°ì²´ ìƒì„± (OpenAI GPT-3.5-turbo)
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

            # LCEL ì²´ì¸ êµ¬ì„±: ì…ë ¥ â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ ë¬¸ìì—´ íŒŒì‹±
            chain = (
                {"question": RunnablePassthrough()} | prompt | llm | StrOutputParser()
            )

            # ì²´ì¸ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ìƒì„±
            assistant_response = chain.invoke(user_input)

            # ì‘ë‹µì„ í•œ ê¸€ìì”© ì¶œë ¥í•˜ì—¬ íƒ€ì´í•‘ íš¨ê³¼ êµ¬í˜„
            for chunk in assistant_response.split():
                full_response += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)

        # ì±—ë´‡ì˜ ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.messages.append(
            {"role": "assistant", "content": full_response}
        )


if __name__ == "__main__":
    main()
