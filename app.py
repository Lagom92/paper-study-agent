import time
import streamlit as st
import tempfile
from dotenv import load_dotenv

# LangChain ë° ì—°ë™ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_core.tools.retriever import create_retriever_tool
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import tools_condition
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

# í”„ë¡œì íŠ¸/íŠ¹ìˆ˜ ëª©ì  ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_teddynote import logging
from langchain_teddynote.messages import random_uuid

from typing import Annotated
from typing_extensions import TypedDict


# PaperState ì •ì˜
class PaperState(TypedDict):
    path: Annotated[str, "FilePath"]
    name: Annotated[str, "FileName"]
    question: Annotated[str, "Question"]
    context: Annotated[str, "Context"]
    answer: Annotated[str, "Answer"]
    messages: Annotated[list, "Messages"]


# PDF íŒŒì¼ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ retriever ìƒì„±
def create_retriever(file_path):
    loader = PyMuPDFLoader(file_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_documents = text_splitter.split_documents(docs)
    embeddings = OpenAIEmbeddings()

    pc = Pinecone()
    index_name = "paper"
    index_names = [index_info["name"] for index_info in pc.list_indexes()]
    if index_name not in index_names:
        pc.create_index(
            name=index_name,
            dimension=1536,
            metric="dotproduct",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        print(f"{index_name} ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    else:
        print(
            f"{index_name} ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì¸ë±ìŠ¤ì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
        )

    vectorstore = PineconeVectorStore.from_documents(
        split_documents, embeddings, index_name=index_name
    )
    print("ë¬¸ì„œê°€ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return vectorstore.as_retriever()


# Chat ë…¸ë“œ
def chat_node(state: PaperState) -> PaperState:
    question = state["question"]
    file_name = state["name"]
    file_path = state["path"]
    pdf_retriever = create_retriever(file_path)
    retriever_tool = create_retriever_tool(
        retriever=pdf_retriever,
        name="pdf_retriever",
        description="Search and return information about PDF file.",
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    llm_tools = llm.bind_tools([retriever_tool])
    prompt = PromptTemplate.from_template(
        """ì œê³µëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜. 
í•œêµ­ë§ë¡œ ëŒ€ë‹µí•´ì¤˜.
# ì œê³µëœ ë¬¸ì„œ ì´ë¦„
{file_name}

# ì§ˆë¬¸:
{question}"""
    )
    chain = prompt | llm_tools
    answer = chain.invoke({"file_name": file_name, "question": question})
    return {"messages": [answer], "answer": answer}


# Retrieval ë…¸ë“œ
def retrieval_node(state: PaperState) -> PaperState:
    question = state["question"]
    file_path = state["path"]
    pdf_retriever = create_retriever(file_path)
    context = pdf_retriever.invoke(question)
    return {"context": context}


# Generation ë…¸ë“œ
def generation_node(state: PaperState) -> PaperState:
    question = state["question"]
    context = state["context"]
    file_name = state["name"]

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = PromptTemplate.from_template(
        """ì œê³µëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜. 
ì°¸ê³  ë‚´ìš©ì˜ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì•Œë ¤ì¤˜.
í•œêµ­ë§ë¡œ ëŒ€ë‹µí•´ì¤˜.
# ì œê³µëœ ë¬¸ì„œ ì´ë¦„
{file_name}

# ì°¸ê³  ë‚´ìš©
{context}

# ì§ˆë¬¸:
{question}"""
    )
    chain = prompt | llm
    answer = chain.invoke(
        {"file_name": file_name, "context": context, "question": question}
    )
    return {"answer": answer}


# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜
def create_graph():
    graph_builder = StateGraph(PaperState)
    graph_builder.add_node("chatbot", chat_node)
    graph_builder.add_node("retriever", retrieval_node)
    graph_builder.add_node("generator", generation_node)
    graph_builder.add_edge(START, "chatbot")
    graph_builder.add_conditional_edges(
        "chatbot",
        tools_condition,
        {
            "tools": "retriever",
            END: END,
        },
    )
    graph_builder.add_edge("retriever", "generator")
    graph_builder.add_edge("generator", END)
    return graph_builder.compile(checkpointer=MemorySaver())


def main():
    load_dotenv()
    logging.langsmith(project_name="Paper-bot")

    st.set_page_config(page_title="Chatbot", page_icon="ğŸ¤–")
    st.title("ğŸ¤– Chatbot")
    st.write("ê¸°ë³¸ LLM ì±—ë´‡")

    with st.sidebar:
        uploaded_file = st.file_uploader(
            "Upload your file", type=["pdf"], accept_multiple_files=False
        )

    if not uploaded_file:
        with st.chat_message("assistant"):
            st.markdown("ë…¼ë¬¸ PDF íŒŒì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_file_name = tmp_file.name

    inputs = PaperState(
        name=uploaded_file.name,
        path=tmp_file_name,
    )

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
        ]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_input := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            config = RunnableConfig(
                recursion_limit=5, configurable={"thread_id": random_uuid()}
            )

            inputs["question"] = user_input
            graph = create_graph()
            outputs = graph.invoke(inputs, config=config)
            chatbot_response = outputs["answer"].content

            # íƒ€ì´í•‘ íš¨ê³¼ ì¶œë ¥
            for char in chatbot_response:
                full_response += char
                time.sleep(0.03)
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)

            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )


if __name__ == "__main__":
    main()
